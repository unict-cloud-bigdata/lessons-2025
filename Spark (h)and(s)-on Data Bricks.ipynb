{
 "cells": [
  {
   "cell_type": "raw",
   "id": "15b24d32-9062-4ca0-8c1c-394806cab263",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Spark (h)and(s)-on Data Bricks\"\n",
    "author: \"Salvo Nicotra\"\n",
    "format: \n",
    "    revealjs:\n",
    "        width: 1280\n",
    "        heigth: 800\n",
    "incremental: true  \n",
    "execute:\n",
    "  echo: true\n",
    "  warning: false\n",
    "theme: white\n",
    "chalkboard: true\n",
    "css: style.css\n",
    "smaller: true\n",
    "scrollable: true\n",
    "include-before: |\n",
    "    <img src=\"images/unict-logo.png\" class=\"custom-logo\" alt=\"Logo\">\n",
    "include-after: |\n",
    "      <div class=\"custom-footer\">\n",
    "        *** Cloud Computing and Big Data - 2024/2025 ***\n",
    "      </div>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a602ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d09a951",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Bricks{background-color=\"white\" background-image=\"https://www.databricks.com/sites/default/files/2023-06/illustration-lakehouse.svg?v=1685694296\"  background-size=\"100%\" background-opacity=\"1\"}\n",
    "<https://www.databricks.com>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f996aa7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 2023 - a Data Lakehouse Platform\n",
    "<https://www.databricks.com/blog/impact-data-and-ai-modern-business>\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"75%\"}\n",
    "**Simple. Open. Multicloud.**\n",
    "\n",
    "The Databricks Lakehouse Platform combines the best elements of **data lakes** and **data warehouses** to deliver the reliability, strong governance and performance of data warehouses with the openness, flexibility and machine learning support of data lakes.\n",
    "\n",
    "This unified approach simplifies your *modern data stack* by eliminating the data silos that traditionally separate and complicate data engineering, analytics, BI, data science and machine learning. \n",
    "\n",
    "It’s built on open source and open standards to maximize flexibility. And, its common approach to data management, security and governance helps you operate more efficiently and innovate faster\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"25%\"}\n",
    "![](https://www.databricks.com/sites/default/files/2023-03/Marketecture_0.svg)\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930a0ebb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Simple\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "The unified approach simplifies your data architecture by eliminating the data silos that traditionally separate analytics, BI, data science and machine learning. \n",
    "\n",
    "With a lakehouse, you can eliminate the **complexity** and **expense** that make it hard to achieve the full potential of your analytics and AI initiatives.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](images/databricks-simple.png){width=400}\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aed34f4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Open\n",
    "<https://www.databricks.com/product/open-source>\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Delta Lake forms the open foundation of the lakehouse by providing reliability and world-record-setting performance directly on data in the data lake. \n",
    "\n",
    "You’re able to avoid proprietary walled gardens, easily share data, and build your modern data stack with unrestricted access to the ecosystem of open source data projects and the broad Databricks partner network.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/e/eb/Open_Source_Initiative.svg){width=300}\n",
    "\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c75ba0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Multicloud\n",
    "<https://www.databricks.com/blog/2021/07/14/petabyte-scale-data-processing-across-multiple-cloud-platforms.html>\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "The Databricks Lakehouse Platform offers you a consistent management, security, and governance experience across all clouds. \n",
    "\n",
    "You don’t need to invest in reinventing processes for every cloud platform that you’re using to support your data and AI efforts. Instead, your data teams can simply focus on putting all your data work to discover new insights.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://www.databricks.com/wp-content/uploads/2021/07/gartner-lake-img-new.png){width=500 .lightbox}\n",
    "\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b696c23",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## 2024 - The Databricks Data Intelligence Platform{background-color=\"white\" background-image=\"https://www.databricks.com/sites/default/files/2023-11/data-intelligence-engine.png?v=1700105416\"  background-size=\"50%\" background-opacity=\"0.7\"}\n",
    "\n",
    ">Data and AI for all\n",
    "\n",
    "<https://www.databricks.com/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047d2b9a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Community Edition \n",
    "<https://community.cloud.databricks.com/login.html>\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "**Create Account**\n",
    "\n",
    "- Sign Up for a new account\n",
    "- Select \"Get started with Community Edition\" instead of selecting a Cloud Provider\n",
    "- Verify the email\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "**Run Tutorial**\n",
    "\n",
    "- Click on Guide: Quickstart tutorial\n",
    "- Create a Cluster clicking on Connect\n",
    "- Run Cells in order\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e21a999",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Apache Spark: Unified engine for large-scale data analytics\n",
    "<https://spark.apache.org>\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Apache Spark™ is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters \n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://spark.apache.org/images/spark-logo-trademark.png)\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a361ba5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Apache Spark project's History\n",
    "\n",
    "> Spark was originally written by the founders of Databricks during their time at UC Berkeley. The Spark project started in 2009, was open sourced in 2010, and in 2013 its code was donated to Apache, becoming Apache Spark. The employees of Databricks have written over 75% of the code in Apache Spark and have contributed more than 10 times more code than any other organization. Apache Spark is a sophisticated distributed computation framework for executing code in parallel across many different machines. While the abstractions and interfaces are simple, managing clusters of computers and ensuring production-level stability is not. Databricks makes big data simple by providing Apache Spark as a hosted solution.\n",
    "\n",
    "A Gentle Introduction to Apache Spark on Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caa7dbb-947e-43b5-b48a-fcc20d55951c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Spark Trends\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "The Spark project started in 2009, was open sourced in 2010, and in 2013 its code was donated to Apache, becoming Apache Spark. \n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](images/SparkTrends.png){.lightbox}\n",
    "\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3035d79a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Genesis of Spark: from Hadoop\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "- Big Data and Distributed Computing at Google (2004)\n",
    "- Hadoop at Yahoo! (2006)\n",
    "- The question then became: there a way to make Hadoop and MR simpler and faster?\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://minimalistquotes.com/wp-content/uploads/2022/08/simple-things-should-be-simple-and-complex-things-.jpg)\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48074aa4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark 1.0 and beyond\n",
    "\n",
    "- **2009** – Spark’s early development begins at UC Berkeley’s AMPLab  \n",
    "- **2010** – First research paper published: Spark shown to be 10–20x faster than MapReduce  \n",
    "- **2014** – Spark 1.0 released: marks the first major production-ready version  \n",
    "- **2016** – Spark 2.0: Introduces DataFrame and Dataset unification, Structured Streaming  \n",
    "- **2020** – Spark 3.0: Adds Hadoop 3.0 support, Pandas UDFs, and a faster SQL engine  \n",
    "- **2023** – Spark 3.4: [Introduces Spark Connect](https://spark.apache.org/docs/latest/spark-connect-overview.html), enabling remote client connectivity  \n",
    "- **2024** – Spark 3.5: Spark Connect goes GA; adds distributed training with DeepSpeed\n",
    "- **2025** – Spark 3.5.5: Maintenance release with key bug fixes, timestamp enhancements, Kubernetes Dockerfile fixes, and REST API hardening \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aa1fea",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Design Philosophy\n",
    "<https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch01.html>\n",
    "\n",
    "Spark’s design philosophy centers around four key characteristics:\n",
    "\n",
    "- Speed\n",
    "- Ease of use\n",
    "- Modularity\n",
    "- Extensibility\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54a043b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1. Speed\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://cc-media-foxit.fichub.com/image/fox-it-mondofox/0177f439-3c0f-44ae-9803-c25f8bfac0dd/flash-vs-superman-game-2jpg-maxw-824.jpg)\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "**Run workloads 100x faster**\n",
    "\n",
    "![Logistic Regression](https://spark.apache.org/images/logistic-regression.png)\n",
    "\n",
    "Apache Spark achieves\n",
    "\n",
    "- high performance for both batch and streaming data\n",
    "- using a state-of-the-art DAG scheduler\n",
    "- a query optimizer\n",
    "- a physical execution engine.\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b072a2f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Why Spark is faster ?\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "   \n",
    "::: {.fragment .column width=\"50%\"}\n",
    "1. Hardware improvements: Today’s commodity servers come cheap, with hundreds of gigabytes of memory, multiple cores, and the underlying Unix-based operating system taking advantage of efficient multithreading and parallel processing.\n",
    "\n",
    "![](https://5.imimg.com/data5/SELLER/Default/2023/10/353578866/VS/YQ/LQ/200119173/top-quality-trimmed-gold-ram-finger-scrap-5-tons-500x500.jpg)\n",
    "[RAM SCRAP](https://m.indiamart.com/proddetail/top-quality-trimmed-gold-ram-finger-scrap-5-tons-2852693519773.html)\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "2.  Direct Acyclic Graph (DAG) Scheduler and Query Optimizer: \n",
    "Provides an efficient computational graph that can usually be decomposed into tasks that are executed in parallel across workers on the cluster\n",
    "\n",
    "![](https://www.researchgate.net/publication/336769100/figure/fig2/AS:817393752371221@1571893265396/Spark-DAG-for-a-WordCount-application-with-two-stages-each-consisting-of-three-tasks.png)\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5928118",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### 2. Ease of Use\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"70%\"}\n",
    "Key factor that influences the success and adoption of a platform  - [Ref](https://www.fastcompany.com/90965452/what-we-mean-when-we-say-ease-of-use)  For Spark\n",
    "\n",
    "- Unified API: Available in Python, Scala, Java, and R, making it accessible to diverse developers.\n",
    "- Interactive Shells: Real-time data exploration and debugging in Python (PySpark) and Scala.\n",
    "- Familiar Data Structures: DataFrame and SQL support simplify data manipulation.\n",
    "- Built-in Libraries: Includes MLlib for machine learning, Spark Streaming, and GraphX, reducing external dependencies.\n",
    "- Lazy Evaluation: Optimizes workflows by delaying computation until necessary.\n",
    "- In-Memory Processing: Speeds up iterative tasks, ideal for data exploration and machine learning.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"30%\"}\n",
    "![](http://www.quickmeme.com/img/4d/4d4759d82ce65de86834ff151bc8b419f89f4e2f0d003f10a54b236785e3e6d2.jpg)\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca237bd7-4c43-45be-ad0b-2527c3951d00",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### 3. Modularity\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "- Write applications quickly in Java, Scala, Python, R, and SQL.\n",
    "- Spark offers over 80 high-level operators that make it easy to build parallel apps. \n",
    "- And you can use it **interactively** from the Scala, Python, R, and SQL shells.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "**Scala Example**\n",
    "\n",
    "```scala\n",
    "df = spark.read.json(\"logs.json\") \n",
    "df.where(\"age > 21\").select(\"name.first\").show()\n",
    "```\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e338131a-e7c7-4ddb-9096-a9e3f07b55b4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### 4 Generality Combine SQL, streaming, and complex analytics.\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Spark powers a stack of libraries including:\n",
    "\n",
    "- **Spark SQL** module for working with structured data\n",
    "- **Spark Streaming** build streaming applications and pipelines\n",
    "- **MLlib** scalable machine learning library\n",
    "- **GraphX** API for graphs and graph-parallel computation\n",
    "New:\n",
    "- **Pandas API**: Use pandas syntax on Spark\n",
    "- **Spark Connect**: Client application that communicate with remote Spark server\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://spark.apache.org/images/spark-stack.png)\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b758a1e-a36b-4aff-ac00-fdaf1a49700b",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Runs everywhere\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://images2.corriereobjects.it/methode_image/socialshare/2014/10/07/f143a1aa-4e22-11e4-b38c-5070a4632162.jpg)\n",
    "\n",
    "<https://www.corriere.it/foto-gallery/esteri/14_ottobre_07/nuovo-attrezzo-fare-sport-ruota-criceti-misura-d-uomo-809cb22a-4e22-11e4-b38c-5070a4632162.shtml>\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "\n",
    "**Spark runs on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud.**\n",
    "You can run Spark using its standalone cluster mode, on EC2, on Hadoop YARN, on Mesos, or on Kubernetes\n",
    "![](https://spark.apache.org/images/spark-runs-everywhere.png)\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746bed81-bcf0-42e4-b15f-5620dee26b66",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## It can access diverse external data sources\n",
    "\n",
    "::: {.fragment}\n",
    "**Analyse**\n",
    "\n",
    "Spark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.\n",
    "\n",
    "<https://spark.apache.org/docs/latest/rdd-programming-guide.html#external-datasets>\n",
    ":::\n",
    "\n",
    "::: {.fragment}\n",
    "**Query**\n",
    "\n",
    "Spark SQL supports operating on a variety of data sources through the DataFrame interface. A DataFrame can be operated on using relational transformations and can also be used to create a temporary view. Registering a DataFrame as a temporary view allows you to run SQL queries over its data.\n",
    "\n",
    "<https://spark.apache.org/docs/latest/sql-data-sources.html>\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305b1322-261d-4708-91eb-8fb9c724d77e",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## In short\n",
    "- Apache Spark is a fast and general-purpose cluster computing system.\n",
    "- It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs.\n",
    "- Supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d45a786-e753-4d19-9965-35525823a679",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Spark (2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f464567-5552-4cc2-a7fc-1436d63460b9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Batch/streaming data\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Unify the processing of your data in batches and real-time streaming, using your preferred language: Python, SQL, Scala, Java or R.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://spark.apache.org/images/batch-sstreaming-data-icon.svg)\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6ea246-17d9-4abb-9a8d-3cc6cace05ef",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## SQL analytics\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Execute fast, distributed ANSI SQL queries for dashboarding and ad-hoc reporting. Runs faster than most data warehouses.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://spark.apache.org/images/sql-analytics-icon.svg)\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21ef3b8-e55b-42a8-b4ac-8aba9286a8dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Data science at scale\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Perform Exploratory Data Analysis (EDA) on petabyte-scale data without having to resort to downsampling\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://spark.apache.org/images/data-science-scale-icon.svg)\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f48064-51fe-4762-9ad2-627dfbe11454",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Machine learning\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Train machine learning algorithms on a laptop and use the same code to scale to fault-tolerant clusters of thousands of machines.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://spark.apache.org/images/machine-learning-icon.svg)\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514272c4-c42d-47e4-bdfa-8ae435131a28",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Examples\n",
    "<https://spark.apache.org/examples.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb1d1e3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Compute Spark PI\n",
    "<https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1408031979081866/1514004441740508/2956912205716139/latest.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abbd584",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Word Count\n",
    "<https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1408031979081866/1177351990298623/2956912205716139/latest.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efa65ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Data Frame Text Search\n",
    "<https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1408031979081866/704214092842249/2956912205716139/latest.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c3e58f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Core Data Abstractions in Spark\n",
    "\n",
    "- Spark provides several key abstractions:  \n",
    "  **RDDs**, **DataFrames**, **Datasets**, and **SQL Tables**\n",
    "\n",
    "- These all represent **distributed collections of data**  \n",
    "  that operate efficiently across clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d793c148",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Immutability in Spark\n",
    "\n",
    "- Spark data structures are **immutable**  \n",
    "  → Once created, they **cannot be changed** directly.\n",
    "\n",
    "- So how do you “modify” data?  \n",
    "  You don’t change it — you **describe how to transform it**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9181ae3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Resilient Distribuited Datasets (RDD)\n",
    "<https://spark.apache.org/docs/latest/rdd-programming-guide.html>\n",
    "\n",
    "[Notebook](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1408031979081866/1455717965234675/2956912205716139/latest.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b1d0d2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Spark SQL, DataFrames and Datasets \n",
    "<https://spark.apache.org/docs/latest/sql-getting-started.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d358f4e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### What is a DataFrame?\n",
    "<https://www.databricks.com/glossary/what-are-dataframes>\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "- A DataFrame is a data structure that organizes data into a 2-dimensional table of rows and columns, much like a spreadsheet. DataFrames are one of the most common data structures used in modern data analytics because they are a flexible and intuitive way of storing and working with data.\n",
    "\n",
    "- Every DataFrame contains a blueprint, known as a schema, that defines the name and data type of each column. Spark DataFrames can contain universal data types like StringType and IntegerType, as well as data types that are specific to Spark, such as StructType. Missing or incomplete values are stored as null values in the DataFrame.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![Dataframe representation](https://www.databricks.com/wp-content/uploads/2018/05/DataFrames.png)\n",
    "\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da27f48c",
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Analogy with Spreadsheet \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "A simple analogy is that a DataFrame is like a spreadsheet with named columns. However, the difference between them is that while a spreadsheet sits on one computer in one specific location, a DataFrame can span thousands of computers. In this way, DataFrames make it possible to do analytics on big data, using distributed computing clusters.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Visicalc.png/440px-Visicalc.png)\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dec1cb8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Originally\n",
    "\n",
    "RDD was the primary user-facing API in Spark since its inception.\n",
    "\n",
    "At the core, an RDD is:\n",
    "\n",
    "- an immutable distributed collection of elements of your data,\n",
    "- partitioned across nodes in your cluster that can be operated in parallel\n",
    "- with a low-level API that offers transformations and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b0edb4-8139-4877-a822-9a90911fa16a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Is RDD enough ?\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://i.imgflip.com/566dsy.jpg)\n",
    "\n",
    "[NicsMeme](https://imgflip.com/i/566dsy)\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://i.imgflip.com/6eibrt.jpg)\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2814d1-3582-4b62-9e6d-f8fa5cee439e",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Spark SQL\n",
    "\n",
    "- Spark SQL is a Spark module for **structured data processing**.\n",
    "- Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations.\n",
    "- There are several ways to interact with Spark SQL including **SQL** and the **Dataset API**.\n",
    "- When computing a result, the same execution engine is used, independent of which API/language you are using to express the computation. This unification means that developers can easily switch back and forth between different APIs based on which provides the most natural way to express a given transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f02c9a-15eb-4635-a1a8-6e514cc6feac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Structured data\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "**Tidy Dataset**\n",
    "\n",
    "> “Happy families are all alike; every unhappy family is unhappy in its own way.” –– Leo Tolstoy\n",
    "\n",
    "> “Tidy datasets are all alike, but every messy dataset is messy in its own way.” –– Hadley Wickham\n",
    "\n",
    "<https://uta.pressbooks.pub/datanotebook/chapter/1-3-structured-data/>\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://uta.pressbooks.pub/app/uploads/sites/85/2021/05/Tidy-Data.png)\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c483bfc6-392a-47ef-87ca-f34b68796348",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### What is structured data?\n",
    "<https://aws.amazon.com/what-is/structured-data/>\n",
    "\n",
    "- Structured data is data that has a standardized format for efficient access by software and humans alike.\n",
    "- It is typically tabular with rows and columns that clearly define data attributes.\n",
    "- Computers can effectively process structured data for insights due to its quantitative nature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98090118-e562-40df-a3ce-c786403c13ad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Structured data in Data Science\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "- Goal 1: Connect steps in the structured data processing pipeline to a real-world case.\n",
    "- Goal 2: Preprocess structured data and prepare features/labels for modeling using pandas.\n",
    "- Goal 3: Understand how Principal Component Analysis can help explore data.\n",
    "- Goal 4: Understand how cross-validation works for time-series data.\n",
    "- Goal 5: Have a general understanding of Decision Tree and Random Forest.\n",
    "- Goal 6: Understand the concept of permutation feature importance.\n",
    "- Goal 7: Experiment with different feature sets and reflect on the choice of features.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://multix.io/structured-data-module/_images/smellpgh-pipeline.png)\n",
    "\n",
    "<https://multix.io/structured-data-module/docs/overview-structured-data.html>\n",
    "\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e4192c-f1af-47d7-ad1a-d3ad5d469799",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## SQL{background-color=\"white\" background-image=\"https://github.com/nicshub/sdsdbms/blob/main/images/sqladam.jpg?raw=true\" background-size=\"50%\" background-opacity=\"0.8\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6267b0-6be7-43ae-a747-a26c66395a3b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Spark SQL\n",
    "- One use of Spark SQL is to execute SQL queries.\n",
    "- Spark SQL can also be used to read data from an existing Hive installation.\n",
    "- When running SQL from within another programming language the results will be returned as a Dataset/DataFrame. \n",
    "- You can also interact with the SQL interface using the command-line or over JDBC/ODBC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0330ad06-db74-45c7-8911-26e01063b3ab",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Dataset\n",
    "\n",
    "- A Dataset is a distributed collection of data. \n",
    "\n",
    "- Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. \n",
    "\n",
    "- A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.). \n",
    "\n",
    "- The Dataset API is available in Scala and Java. Python does not have the support for the Dataset API. But due to Python’s dynamic nature, many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally row.columnName). The case for R is similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99913e6b-720f-406e-88c9-be2567b9ba02",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Data Frame\n",
    "- A DataFrame is a Dataset organized into named columns. \n",
    "\n",
    "- It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. \n",
    "\n",
    "- DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. \n",
    "\n",
    "- The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d5ecd1-a638-427a-bf42-1aa4adf57a28",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Yet another definitions of DataFrames \n",
    "<https://www.databricks.com/glossary/what-are-dataframes>\n",
    "\n",
    "The concept of a DataFrame is common across many different languages and frameworks. DataFrames are the main data type used in pandas, the popular Python data analysis library, and DataFrames are also used in R, Scala, and other languages.\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "- Every DataFrame contains a blueprint, known as a schema, that defines the name and data type of each column. \n",
    "- Spark DataFrames can contain universal data types like StringType and IntegerType, as well as data types that are specific to Spark, such as StructType. \n",
    "- Missing or incomplete values are stored as null values in the DataFrame.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2019/04/new-1-768x515.png)\n",
    "<https://www.edureka.co/blog/dataframes-in-spark/>\n",
    "\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3403fc85-bc22-4edb-9670-6b0b96cb2679",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Dataframe efficency\n",
    "\n",
    "![](https://databricks.com/wp-content/uploads/2016/07/memory-usage-when-caching-datasets-vs-rdds.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ad9208-520f-496e-af0a-4af9bd2f9888",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Evolution\n",
    "\n",
    "::: {.r-stack}\n",
    "![](images/human-evolution-monkey-modern-man-programmer-computer-user-isolated-white_33099-1593.jpg){.fragment}\n",
    "\n",
    "![](https://databricks.com/wp-content/uploads/2016/06/Unified-Apache-Spark-2.0-API-1.png){.fragment}\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904c3fab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Data Frame Examples\n",
    "\n",
    "[EDA](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1408031979081866/3119543398385477/2956912205716139/latest.html)\n",
    "\n",
    "[Flights](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1408031979081866/4241690966276695/2956912205716139/latest.html)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": "true",
   "footer": "<div class=\"tswd-footer\"> *** Cloud Computing and Big Data - 2024 ***</div>",
   "header": "<div class=\"tswd-header\"></div>",
   "scroll": true,
   "theme": "night"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
